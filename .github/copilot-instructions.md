# Copilot Instructions for C2RustAgent

C2RustAgent is a **hybrid intelligent C-to-Rust transpiler** combining formal static analysis with LLM semantic enhancement. The system transforms C code into safe, idiomatic Rust through a multi-stage pipeline while maintaining correctness guarantees.

## Architecture Overview

### Dual System Design

This repository contains **two complementary translation approaches**:

**Main System** (`src/`) - Formal analysis-based

- **Pipeline**: C Source → Clang AST → MIR → Static Analysis → Rust Codegen
- **Status**: Core architecture complete, analysis passes in development
- **Use case**: Large C projects requiring strong type safety guarantees
- **Key strength**: Extensible analysis framework with serializable MIR

**Hybrid Subsystem** (`translate_hybrid/`) - LLM-first approach

- **Pipeline**: C → LLM Translation → Iterative Syntax Fixing → unsafe Optimization
- **Status**: ✅ Production-ready (945 LOC, 12/12 tests passing on chibicc)
- **Use case**: Medium-sized projects, rapid prototyping, **batch project translation**
- **Key strength**: Large context window (1049K), streaming responses, iterative repair, **project-level translation**
- **New feature**: `translate-project` command for batch translation of entire C projects

**Selection guide**: Use `translate_hybrid/` for fast conversion or batch projects; use main system for complex analysis

### Design Philosophy

```
C Code → Clang AST → MIR → Analysis Pipeline → Rust Code
            ↓         ↓          ↓                ↓
        LLM Semantic Co-processing (throughout)
```

**Core principle**: Formal methods guarantee correctness (borrow checking, type safety), LLM enhances readability (comments, naming, ownership inference)

**Critical pattern**: Two-phase AST→MIR conversion solves forward reference issues; all LLM functions have mock fallbacks for testability

## Key Implementation Patterns

### 1. Project Loading (`src/project_loader.rs`)

```rust
// Entry point: Load C project from compile_commands.json
let project = CProject::load(&root)?;

// Callback-based processing avoids memory pressure
project.process_units(|spec, tu| {
    // Process each translation unit on-demand
    Ok(())
})?;
```

**Critical detail**: Uses `compile_commands.json` generated by `bear -- make` or CMake

### 2. Two-Phase AST→MIR Conversion (`src/ast_to_mir.rs`)

```rust
// Phase 1: Symbol Discovery (signatures only, no bodies)
fn discover_symbols(project: &CProject, out: &mut ProjectMIR) -> Result<()>

// Phase 2: Body Conversion (build basic blocks, fill CFG)
fn convert_bodies(project: &CProject, out: &mut ProjectMIR) -> Result<()>
```

**Why two phases?**: Avoids forward reference issues when functions call each other
**State management**: Maintains `var_map: HashMap<String, VarId>` and `next_var_id` for variable scope

### 3. Analysis Framework (`src/analysis/mod.rs`)

```rust
// Centralized analysis orchestration
impl AnalysisManager<'_> {
    pub fn run_all_passes(&self) -> ProjectAnalysisResults {
        let mut results = HashMap::new();
        for (name, func) in &self.project_mir.functions {
            // Run all analysis passes
            let liveness = run_liveness_analysis(func);
            // Future: pointer analysis, ownership inference, etc.
            results.insert(name.clone(), PerFunctionResults { liveness });
        }
        ProjectAnalysisResults { results }
    }
}
```

**Extension pattern**: Add new analysis by (1) creating `analysis/your_analysis.rs`, (2) adding field to `PerFunctionResults`, (3) calling in `run_all_passes()`

### 4. Code Generation (`src/codegen.rs`)

```rust
// Baseline generation (no LLM)
pub fn generate(&mut self, mir: &ProjectMIR, analysis: &Results) -> Result<()>

// LLM-enhanced generation (async)
pub async fn generate_with_llm(&mut self, mir: &ProjectMIR, analysis: &Results) -> Result<()>
```

**Module strategy**: Groups by source file, generates separate `globals.rs` for global variables
**Key method**: `build_source_module_mapping()` determines module boundaries

### 5. LLM Integration (`src/llm_assists.rs`)

```rust
// All LLM functions are async with graceful degradation
pub async fn infer_external_api_semantics(func: &Function) -> Vec<String> {
    match call_llm_api(...).await {
        Ok(resp) => parse_semantic_tags(resp),
        Err(_) => infer_external_api_semantics_mock(func), // Rule-based fallback
    }
}
```

**Semantic annotation system**:

- `[ReturnsNewResource(free)]` → Rust `impl Drop`
- `[TakesOwnership(param)]` → `fn(param: Box<T>)`
- `[RequiresNonNull(param)]` → `NonNull<T>`

## Development Workflows (Windows)

### Environment Setup (One-time)

```powershell
# 1. Install LLVM (required for libclang)
# Download from: https://github.com/llvm/llvm-project/releases
# Set environment variable after installation:
$env:LIBCLANG_PATH = "C:\Program Files\LLVM\bin"

# 2. Configure LLM API (priority: user config > env var > project config)
# Method 1: User config (recommended)
cargo run --bin c2rust-agent-config -- init
notepad $env:APPDATA\c2rust-agent\config.toml  # Edit api_key and model

# Method 2: Environment variable
$env:OPENAI_API_KEY = "sk-your-key"

# Method 3: Project config (DO NOT commit with API keys!)
cargo run --bin c2rust-agent-config -- init-project
```

### Daily Commands

```powershell
# === Main System ===
cargo build                                    # Build project
cargo run                                      # Demo mode (builtin C → AST + MIR)
cargo run -- ./translate_littlefs_fuse        # Translate C project
cargo test                                     # Run tests (LLM auto-mocked)
cargo fmt && cargo clippy                      # Pre-commit checks

# === Config Management CLI ===
cargo run --bin c2rust-agent-config -- show          # View effective config
cargo run --bin c2rust-agent-config -- show --verbose # Show config sources
cargo run --bin c2rust-agent-config -- validate      # Validate config
cargo run --bin c2rust-agent-config -- path          # Show config file path

# === translate_hybrid Subsystem ===
cd translate_hybrid
cargo run -- init                                      # Initialize config
cargo run -- test-llm --prompt "Hello"                # Test LLM connection
cargo run -- translate -i input.c -o output.rs         # Translate single file
cargo run -- translate-project -p ../project -o ../out # Translate entire project
cargo run -- optimize-unsafe -i code.rs                # Optimize unsafe blocks
cargo test                                             # Run unit tests

# Quick project translation (using script)
.\translate_chibicc_project.ps1                        # Translate chibicc project
```

### Testing C Project Translation

```powershell
# Example 1: Translate littlefs-fuse (950 functions)
cd translate_littlefs_fuse
# Verify compile_commands.json exists
cargo run -- .

# Example 2: Custom C project
cd your_c_project
bear -- make  # Generate compile_commands.json
# Or: cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON .
cargo run -- /path/to/your_c_project
```

## Conventions & Patterns

### MIR Design Principles

- **Basic blocks**: Each `BasicBlock` = `statements[]` + single `Terminator`
- **L-values vs R-values**: `LValue` (assignment target) vs `RValue` (expression)
- **Serialization**: All MIR structs implement `serde::Serialize` for JSON export
- **LLM annotations**: `Function.annotations: Vec<String>` stores semantic tags

### Error Handling

```rust
// Use anyhow::Result with context
pub fn convert_project(project: &CProject) -> Result<ProjectMIR> {
    let mir = convert_mir(project)
        .with_context(|| "Failed to convert project to MIR")?;
    Ok(mir)
}

// Avoid unwrap() in production code (demo code is OK)
```

### Async LLM Pattern

```rust
// All LLM functions:
// 1. Are async
// 2. Return concrete types (not Result)
// 3. Have mock fallbacks
pub async fn infer_ownership(func: &Function) -> Vec<String> {
    match call_llm_api(...).await {
        Ok(resp) => parse_tags(resp),
        Err(_) => mock_inference(func), // Graceful degradation
    }
}

// Testing async functions
#[tokio::test]
async fn test_llm_inference() {
    let result = infer_ownership(&func).await;
    assert!(!result.is_empty());
}
```

### Dependency Management

**Main System** (`Cargo.toml`):

- `clang` + `clang-sys` - C AST parsing
- `tokio` + `async-openai` - Async LLM calls
- `anyhow` / `thiserror` - Error handling
- `config` + `dirs` - Configuration management

**translate_hybrid** (`translate_hybrid/Cargo.toml`):

- `reqwest` - HTTP client with streaming
- `serde_json` - JSON parsing
- `console` - Colored output (see `utils.rs`)
- `tracing` - Structured logging

### Testing Strategy

1. **Unit tests**: Use `#[cfg(test)] mod tests` in modules
2. **Integration tests**: Use `tempfile` for temporary directories
3. **LLM mocking**: `USE_MOCK_LLM=true` automatically enables mocks
4. **Async testing**: Use `#[tokio::test]` attribute macro
5. **Output verification**: Generated Rust should pass `cargo check`

## Common Tasks

### Adding a Static Analysis Pass

```rust
// 1. Create src/analysis/pointer_analysis.rs
pub struct PointerAnalysisResult {
    pub origins: HashMap<VarId, Origin>,
}

pub fn run_pointer_analysis(func: &Function) -> PointerAnalysisResult {
    // Implement analysis
    PointerAnalysisResult { origins: HashMap::new() }
}

// 2. Register in src/analysis/mod.rs
pub struct PerFunctionResults {
    pub liveness: LivenessResult,
    pub pointer_origins: PointerAnalysisResult, // Add field
}

impl AnalysisManager<'_> {
    pub fn run_all_passes(&self) -> ProjectAnalysisResults {
        // ...
        per_fn.pointer_origins = run_pointer_analysis(func); // Add call
    }
}
```

### Extending LLM Inference

```rust
// Add to src/llm_assists.rs
pub async fn infer_lifetime_annotations(func: &Function) -> Vec<String> {
    let prompt = format!("Infer lifetimes for MIR:\n{:?}", func);
    match call_llm_api(&prompt, ...).await {
        Ok(resp) => parse_lifetime_tags(resp),
        Err(_) => mock_lifetime_inference(func), // Must provide mock!
    }
}

// Mock implementation for testing
fn mock_lifetime_inference(func: &Function) -> Vec<String> {
    // Rule-based heuristics
    vec!["'a".to_string()]
}
```

## Debugging Tips

- **Inspect MIR**: Use `serde_json::to_string_pretty(&function)?` to see structure
- **Force mock mode**: `$env:USE_MOCK_LLM="true"; cargo test`
- **View AST**: Check `traverse_ast()` in `src/main.rs`
- **Clang issues**: Verify `LIBCLANG_PATH` and `compile_commands.json` format
- **LLM timeouts**: Increase `timeout` in config or use mock mode
- **UTF-8 issues**: translate_hybrid handles this; main system uses `async-openai`

## Troubleshooting

### Main System Issues

1. **LLVM not found**:

   ```powershell
   $env:LIBCLANG_PATH = "C:\Program Files\LLVM\bin"
   ```

2. **Invalid LLM config**:

   ```powershell
   cargo run --bin c2rust-agent-config -- show --verbose
   cargo run --bin c2rust-agent-config -- validate
   ```

3. **MIR conversion fails**:
   - Check `compile_commands.json` exists
   - Verify C project compiles: `bear -- make`
   - See `docs/P4提示词.md` for known type mapping limitations

### translate_hybrid Issues

1. **Streaming interrupted**: Check network, increase `timeout` in config
2. **High unsafe ratio**: Use `cargo run -- optimize-unsafe <file>`
3. **Rust compile errors**: Use iterative repair with `cargo check` + LLM fix loops

## Reference Documentation

- Architecture: `docs/CORE_DESIGN.md`
- Success case: `docs/reports/TRANSLATION_SUCCESS_SUMMARY.md` (recommended read)
- Example projects: `translate_chibicc/` (verified), `translate_littlefs_fuse/` (in progress)
- Docker: `docs/docker/DOCKER_QUICKREF.md`

## Critical Notes

⚠️ **Active development** - API may change frequently  
⚠️ **Security** - Never commit `c2rust-agent.toml` with API keys  
⚠️ **LLVM required** - Windows users must install LLVM and set LIBCLANG_PATH
